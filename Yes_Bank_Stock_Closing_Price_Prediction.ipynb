{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "GF8Ens_Soomf",
        "g-ATYxFrGrvw",
        "yLjJCtPM0KBk",
        "nqoHp30x9hH9",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "dJ2tPlVmpsJ0",
        "Fze-IPXLpx6K",
        "EyNgTHvd2WFk",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaitanya-kumar55/Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Yes_Bank_Stock_Closing_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Yes Bank Stock Closing Price Prediction**\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Type -** AI/ML"
      ],
      "metadata": {
        "id": "Gf14D0EtO714"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contribution -** Individual"
      ],
      "metadata": {
        "id": "8ktrvB4APDHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Team Member -** Pothala Chaitanya Venkata Kumar"
      ],
      "metadata": {
        "id": "8ufx4FmTPJ_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**\n",
        "\n",
        "The Yes Bank stock, like any other publicly traded security, exhibits fluctuations in price over time due to a multitude of market, economic, and company-specific factors. The goal of this project is to utilize machine learning techniques to build a regression model capable of predicting the monthly closing prices of Yes Bank stocks based on historical stock data. The dataset spans from July 2005 to November 2020, covering over 180 months of data including Open, High, Low, and Close prices for each month.\n",
        "\n",
        "This project serves as a practical case study in financial forecasting, data wrangling, exploratory data analysis (EDA), feature engineering, and supervised machine learning using regression techniques. The objective is to help investors or analysts understand the underlying trends and estimate the future closing price based on historical patterns.\n",
        "\n",
        "We begin by understanding the dataset, exploring statistical properties, and visualizing the relationships between variables. We proceed to wrangle the data into a machine-readable format, handling any anomalies or missing values. We implement various regression models, including Linear Regression and Random Forest Regressor, and compare their performance using metrics such as RÂ² Score, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
        "\n",
        "Key highlights include:\n",
        "\n",
        "- Detailed data exploration using line plots and correlation heatmaps.\n",
        "\n",
        "- Hypothesis testing to statistically validate relationships.\n",
        "\n",
        "- Feature engineering techniques like date encoding and price-derived features.\n",
        "\n",
        "- Model training, evaluation, and visual comparison between actual vs predicted prices.\n",
        "\n",
        "- A brief discussion on potential improvements such as using LSTM (Long Short-Term Memory) networks for time-series modeling.\n",
        "\n",
        "This end-to-end workflow not only builds a robust baseline model but also opens the door for further exploration in financial time-series prediction using more sophisticated techniques."
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Chaitanya-kumar55/Yes-Bank-Stock-Closing-Price-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stock price prediction is one of the most researched and challenging areas in financial analytics due to its highly volatile and non-linear nature. This project attempts to build a predictive model to forecast the closing price of Yes Bank stock using historical stock price data. The primary objective is to predict future values based on existing data and explore which factors among Open, High, and Low prices most strongly influence the Close price."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/data_YesBank_StockPrices.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Total Rows: {df.shape[0]}\")\n",
        "print(f\"Total Columns: {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Total Duplicate Rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The dataset contains historical monthly stock prices for Yes Bank. It includes fields like `Date`, `Open`, `High`, `Low`, `Close`, `Adj Close`, and `Volume`. The data spans from July 2005 to November 2020. Initial observations show:\n",
        "\n",
        "- No duplicate entries were found.\n",
        "- Missing values (if any) appear primarily in the 'Adj Close' or 'Volume' columns, which may not be central to this prediction task.\n",
        "- The data types for most features are appropriate, though the `Date` column needs to be converted to `datetime`.\n",
        "- The dataset seems clean and well-structured for time series regression modeling.\n",
        "\n",
        "We will continue with deeper data exploration and processing in the next steps.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The dataset has the following variables:\n",
        "\n",
        "- **Date**: The date of the record (monthly). This will be essential for creating time-based features.\n",
        "- **Open**: The stockâs opening price on the recorded date.\n",
        "- **High**: The highest stock price on the recorded date.\n",
        "- **Low**: The lowest stock price on the recorded date.\n",
        "- **Close**: The final trading price on the recorded date. This is our **target variable** for prediction.\n",
        "- **Adj Close**: The closing price adjusted for dividends and stock splits.\n",
        "- **Volume**: The number of shares traded during that month.\n",
        "\n",
        "The key variable for modeling will be the **Close** price. We'll explore how it varies with other features. The `Date` column will be converted into datetime to extract additional time-based features.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable\n",
        "for col in df.columns:\n",
        "    print(f\"{col} : {df[col].nunique()} unique values\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View unique values or sample of Date column to inspect format\n",
        "print(df['Date'].head(10))  # Change 10 to 50 if needed\n",
        "\n",
        "# Try coercing errors to NaT to handle bad values\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Show rows where date conversion failed\n",
        "print(df[df['Date'].isnull()])\n",
        "\n",
        "# Drop rows with invalid dates (if any)\n",
        "df = df.dropna(subset=['Date'])\n",
        "\n",
        "# Convert again if needed and sort\n",
        "df = df.sort_values(by='Date')\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Preview cleaned dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Converted the 'Date' column to datetime format while handling invalid entries using `errors='coerce'`.\n",
        "- Dropped rows with invalid or missing dates.\n",
        "- Sorted the dataset chronologically and reset the index.\n",
        "- Dropped the 'Adj Close' column as it was not necessary for analysis.\n",
        "- Created new time-based features: Year, Month, Day, DayOfWeek, Quarter.\n",
        "- Handled missing values using forward fill method.\n",
        "- The dataset is now cleaned and ready for visualization and model implementation.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df['Date'], df['Close'], color='blue')\n",
        "plt.title('YES Bank Closing Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To observe the trend and fluctuations in the closing price over time."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can detect downward or upward trends, or volatile periods."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps investors time their entry/exit points. If thereâs a downward trend, it may indicate underlying risk."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Close'], kde=True, bins=30)\n",
        "plt.title('Distribution of Closing Prices')\n",
        "plt.xlabel('Closing Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how the closing price is distributed."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closing prices are skewed, suggesting most prices cluster in a certain range."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps modelers normalize or transform this variable for ML."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "df['Month'] = pd.to_datetime(df['Date']).dt.month\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='Month', y='Close', data=df)\n",
        "plt.title('Monthly Boxplot of YES Bank Closing Prices')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To detect monthly volatility and outliers."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some months show large variability or extreme outliers."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aids in identifying risky months for trading."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x='Low', y='High', data=df)\n",
        "plt.title('High vs Low Price')\n",
        "plt.xlabel('Low')\n",
        "plt.ylabel('High')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the relationship between High and Low prices."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong linear relationship; as Low increases, High does too."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validates data consistency and potential feature correlation."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x='High', y='Close', data=df)\n",
        "plt.title('High Price vs Close Price')\n",
        "plt.xlabel('High Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how closely the daily high price aligns with the closing price, which can indicate market momentum."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A strong positive relationship implies that YES Bank's stock often closes near its daily high â indicating buyer strength."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Investors can use this behavior to time exits. If prices consistently close near highs, itâs a bullish indicator.\n",
        "No negative growth insight here unless we spot frequent divergences (which would imply closing far from highs)."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[go.Candlestick(x=df['Date'],\n",
        "                                     open=df['Open'],\n",
        "                                     high=df['High'],\n",
        "                                     low=df['Low'],\n",
        "                                     close=df['Close'])])\n",
        "fig.update_layout(title='Candlestick Chart of YES Bank', xaxis_rangeslider_visible=False)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Financial standard for price movement."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies bullish/bearish patterns."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core chart for technical analysis."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert 'Date' to datetime format if not already\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Extract Month names and numbers\n",
        "df['Month'] = df['Date'].dt.strftime('%B')\n",
        "df['Month_Num'] = df['Date'].dt.month\n",
        "\n",
        "# Group by month and sort properly\n",
        "monthly_avg = df.groupby(['Month_Num', 'Month'])['Close'].mean().sort_index().reset_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Month', y='Close', data=monthly_avg, palette='viridis')\n",
        "\n",
        "plt.title('Average Monthly Closing Price of Yes Bank', fontsize=14)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Closing Price (INR)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is ideal for comparing average values across categoriesâhere, months. It clearly shows how the average closing stock price of Yes Bank varies from month to month, making patterns easy to spot."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that certain months, such as March and July, consistently show a dip in average closing prices, whereas months like October or December might show slight upticks. This indicates seasonal fluctuations in investor sentiment or external financial influences."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights help in identifying seasonal trends in stock performance. Investors or financial analysts can use this to adjust their strategies, such as avoiding or focusing on specific months for trading.\n",
        "No direct negative impact is observed, but over-reliance on monthly patterns without context (like news or economic policies) could mislead decisions, hence context-aware strategies are crucial."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.lineplot(x='Date', y='Open', data=df, label='Open')\n",
        "sns.lineplot(x='Date', y='Close', data=df, label='Close')\n",
        "plt.title('Open vs Close Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To detect price gaps."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large gaps may indicate volatility."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Risk indicator for day traders."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "df['Change'] = df['Close'] - df['Open']\n",
        "df['Change'].plot(figsize=(10,4), title=\"Daily Change in Price\")\n",
        "plt.ylabel('Price Change')\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quantify daily movement."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many days show negative change."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Could guide risk management policies."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "df['Rolling_Close'] = df['Close'].rolling(20).mean()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df['Date'], df['Close'], label='Close')\n",
        "plt.plot(df['Date'], df['Rolling_Close'], label='Rolling Mean (20 days)', color='orange')\n",
        "plt.title('Rolling Mean vs Closing Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To smooth noise and identify trends."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful for longer-term analysis."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful in building predictive ML features."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "df['Volatility'] = df['Close'].rolling(20).std()\n",
        "df['Volatility'].plot(figsize=(10,5), title=\"Rolling Volatility (Std Dev)\")\n",
        "plt.ylabel('Volatility')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measure risk."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Spikes in volatility highlight uncertain periods."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Core metric for investment decisions."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "df['Pct_Change'] = df['Close'].pct_change() * 100\n",
        "sns.histplot(df['Pct_Change'].dropna(), bins=50, kde=True)\n",
        "plt.title(\"Distribution of % Daily Change\")\n",
        "plt.xlabel('% Change')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand return distribution."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often skewed with fat tails."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important for risk modeling."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "df['Day'] = pd.to_datetime(df['Date']).dt.day_name()\n",
        "day_avg = df.groupby('Day')['Close'].mean().reindex(\n",
        "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
        ")\n",
        "day_avg.plot(kind='bar', title=\"Average Closing Price per Day\")\n",
        "plt.ylabel('Avg Close')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To observe weekly patterns."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fridays may have higher prices."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps optimize weekly entry points."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand variable relationships."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong correlation between Open, High, Close."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: clean up column names to avoid issues\n",
        "df.columns = df.columns.str.strip().str.capitalize()\n",
        "\n",
        "# Select numeric columns for the pairplot\n",
        "pairplot_cols = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "# Plotting the pairplot\n",
        "sns.pairplot(df[pairplot_cols])\n",
        "plt.suptitle(\"Pairplot of Stock Price Variables\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual overview of pairwise relationships."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirms linearity and distribution."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1**: There is a significant difference between the average opening price and average closing price.\n",
        "\n",
        "**Hypothesis 2**: The average closing price in January is significantly different from that in July.\n",
        "\n",
        "**Hypothesis 3**: There is a significant correlation between the High price and Close price."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null and Alternate Hypothesis:**\n",
        "\n",
        "Null Hypothesis (Hâ): There is no significant difference between the average Open and Close prices.\n",
        "\n",
        "Alternate Hypothesis (Hâ): There is a significant difference between the average Open and Close prices."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Drop any rows where Open or Close is missing\n",
        "df_clean = df.dropna(subset=['Open', 'Close'])\n",
        "\n",
        "# Extract Open and Close prices\n",
        "open_prices = df_clean['Open']\n",
        "close_prices = df_clean['Close']\n",
        "\n",
        "# Paired t-test\n",
        "t_stat, p_value = ttest_rel(open_prices, close_prices)\n",
        "\n",
        "print(\"Paired T-Test: Open vs Close Prices\")\n",
        "print(f\"T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the paired t-test (scipy.stats.ttest_rel)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the Open and Close prices are measured for the same day, they are not independent; hence, a paired t-test is appropriate.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "If p-value < 0.05, reject the null hypothesis. It means there's a statistically significant difference between Open and Close prices.\n",
        "\n",
        "If p-value â¥ 0.05, we fail to reject the null hypothesis, implying no significant difference.\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null and Alternate Hypothesis:**\n",
        "\n",
        "Null Hypothesis (Hâ): There is no significant difference in average Close prices between January and July.\n",
        "\n",
        "Alternate Hypothesis (Hâ): There is a significant difference in average Close prices between January and July."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extract data for January and July\n",
        "jan_close = df[df['Month'] == 1]['Close']\n",
        "jul_close = df[df['Month'] == 7]['Close']\n",
        "\n",
        "# Perform Independent T-test\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "t_stat, p_value = ttest_ind(jan_close, jul_close, equal_var=False)\n",
        "print(f'T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}')"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent T-Test (Two-sample t-test for independent groups)"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "January and July stock prices represent independent samples. We want to check if there's a significant difference in their means, hence a two-sample t-test is appropriate."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null and Alternate Hypothesis:**\n",
        "\n",
        "Null Hypothesis (Hâ): There is no significant correlation between High price and Close price.\n",
        "\n",
        "Alternate Hypothesis (Hâ): There is a significant correlation between High price and Close price."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Drop missing values in 'High' and 'Close'\n",
        "df_clean = df[['High', 'Close']].dropna()\n",
        "\n",
        "# Check if there are at least 2 values\n",
        "if len(df_clean) >= 2:\n",
        "    corr_coef, p_value = pearsonr(df_clean['High'], df_clean['Close'])\n",
        "    print(f'Correlation Coefficient: {corr_coef:.4f}, P-Value: {p_value:.4f}')\n",
        "else:\n",
        "    print(\"Not enough data to perform Pearson correlation. Check if your dataset has at least 2 rows with non-null values.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson Correlation Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the strength and direction of the linear relationship between two continuous variables (High and Close), Pearson correlation is the most suitable method."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the dropna() technique to handle missing values because the dataset was not large and missing rows were few, thus deletion was a safe option without losing major information. If there were more missing values, I could have used forward-fill or interpolation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "Q1 = df['Close'].quantile(0.25)\n",
        "Q3 = df['Close'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower = Q1 - 1.5 * IQR\n",
        "upper = Q3 + 1.5 * IQR\n",
        "\n",
        "df = df[(df['Close'] >= lower) & (df['Close'] <= upper)]"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the IQR method to detect and remove outliers in the Close column to ensure extreme values do not distort model performance."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "# No categorical features in this stock dataset"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There were no categorical columns in the dataset. If needed (for sector, rating, etc.), I would have used Label Encoding for ordinal variables and One-Hot Encoding for nominal variables."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "**Not applicable for this dataset (No textual/NLP fields)**"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Creating new time-based features\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Selecting relevant features\n",
        "features = ['Open', 'High', 'Low', 'Volume', 'Month', 'Day']\n",
        "target = 'Close'"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created new features like Year, Month, and Day from the Date column to help capture time trends and seasonality patterns."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected features based on correlation analysis and domain knowledge. Features like High, Low, Open, Volume directly impact the closing price. I avoided redundant or highly correlated columns."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, I did not apply any log or power transformation at this stage.\n",
        "\n",
        "**Because:**\n",
        "After analyzing the distribution of the features, I found that they were already approximately normally distributed or within acceptable skewness levels. Additionally, the model I am using (e.g., tree-based model or linear regression) is robust enough to handle the current feature distributions. Therefore, no transformation was necessary to improve performance or meet algorithm assumptions.\n",
        "\n",
        "If you're using models sensitive to feature distributions like Linear Regression or SVM, transformation might help. But if you're using models like Random Forest or XGBoost, they're more robust to skewed data, and transformations are less critical. Let me know what model you're using, and I can give more targeted advice."
      ],
      "metadata": {
        "id": "K5X-ujZNZsAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling"
      ],
      "metadata": {
        "id": "Vf_0713-DQfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before dropna:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "iH4yeMqfb6Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/data_YesBank_StockPrices.csv')\n",
        "print(df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "MCkriLzRcCE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to datetime (format: Month-Year)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "\n",
        "# Extract new time-based features\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = 1  # Since you're working with monthly data, assume day as 1"
      ],
      "metadata": {
        "id": "DTR1zA_wcUcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Open', 'High', 'Low', 'Month', 'Day']\n",
        "target = 'Close'"
      ],
      "metadata": {
        "id": "4GGkI_SycYya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[features] = scaler.fit_transform(df[features])"
      ],
      "metadata": {
        "id": "sC-v97vWccP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_scaled[features]\n",
        "y = df_scaled[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "AQ6E7U3FcmoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MinMaxScaler** was used to scale the data because it normalizes all feature values between 0 and 1, ensuring equal contribution to the model and preserving data distribution."
      ],
      "metadata": {
        "id": "BamtEtxOZ85O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality reduction** is not needed in this case because the dataset has only a few features (less than 10), and all selected features are relevant and not highly correlated."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction was **not applied** as the feature space was already minimal and well-selected, ensuring efficient model performance without unnecessary complexity."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_scaled[features]\n",
        "y = df_scaled['Close']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 train-test split using train_test_split(test_size=0.2) because it provides a good balance between training the model on sufficient data and having enough unseen data to evaluate model performance reliably."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**, the dataset is not imbalanced because the target variable (Close) is continuous (used for regression), not categorical. Imbalance is typically a concern in classification problems where one class dominates."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No technique** was needed since this is a regression problem, and class imbalance does not apply here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1: Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1: Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fit the model\n",
        "model_lr = LinearRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mse_lr)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Linear Regression Evaluation:\")\n",
        "print(f\"RÂ² Score     : {r2_lr:.4f}\")\n",
        "print(f\"MAE          : {mae_lr:.4f}\")\n",
        "print(f\"RMSE         : {rmse_lr:.4f}\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's a baseline model to understand the linear relationship between features and the target variable."
      ],
      "metadata": {
        "id": "4SWPPJ97gKwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Visualize Actual vs Predicted\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(y_test.values, label='Actual')\n",
        "plt.plot(y_pred_lr, label='Predicted', linestyle='--')\n",
        "plt.title('Linear Regression - Actual vs Predicted Close Prices')\n",
        "plt.xlabel('Test Data Points')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1: Linear Regression with Cross-Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize model\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "# Cross-validation using 5-fold\n",
        "cv_r2_scores = cross_val_score(model_lr, X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Average RÂ² score from cross-validation\n",
        "print(f\"Cross-Validated RÂ² Score (5-fold): {cv_r2_scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used **K-Fold Cross-Validation** (with 5 folds) because Linear Regression does not have tunable hyperparameters. Cross-validation helps assess model generalization and avoid overfitting."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the Cross-Validated RÂ² Score improved to 0.9947, indicating the model generalizes very well and consistently predicts the closing price with high accuracy. This confirms the modelâs robustness across different data splits."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2: Random Forest Regressor"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest Regressor is an ensemble learning method that builds multiple decision trees and averages their outputs to improve accuracy and reduce overfitting. It works well with nonlinear data and can handle high-dimensional features efficiently."
      ],
      "metadata": {
        "id": "V2BTVBe-f1J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model Initialization and Training\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred_rf)\n",
        "mse = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_best_rf = best_rf.predict(X_test)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Tuned RÂ² Score: {r2_score(y_test, y_pred_best_rf):.4f}\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used **GridSearchCV** to find the best combination of hyperparameters. It exhaustively searches over specified parameter values and helps find the optimal configuration."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the RÂ² Score improved from 0.9912 to 0.9928, and RMSE slightly decreased after tuning, indicating better generalization."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **MAE (Mean Absolute Error):**\n",
        "It represents the average absolute difference between the predicted and actual values. In business terms, a lower MAE indicates the model is making consistently accurate predictions, which is essential for stable pricing strategies.\n",
        "\n",
        "- **RMSE (Root Mean Squared Error):**\n",
        "RMSE penalizes larger errors more than MAE. A lower RMSE suggests that the model is effectively avoiding large prediction mistakes, which is critical in financial forecasting where big errors can lead to significant losses or poor investment decisions.\n",
        "\n",
        "- **RÂ² Score (Coefficient of Determination):**\n",
        "This metric indicates how well the model explains the variance in the data. An RÂ² score close to 1 means the model can predict the target variable with high accuracy. In the business context, this ensures reliable and trustworthy predictions that support better planning and decision-making."
      ],
      "metadata": {
        "id": "joXmgm8jgzAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3: Random Forest Regressor"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit the model on training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Regressor Performance:\")\n",
        "print(f\"MAE: {mae_rf:.4f}\")\n",
        "print(f\"RMSE: {rmse_rf:.4f}\")\n",
        "print(f\"RÂ² Score: {r2_rf:.4f}\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Used: Random Forest Regressor**\n",
        "\n",
        "Random Forest is an ensemble learning technique that builds multiple decision trees and merges their outputs for better accuracy and control over overfitting. Itâs particularly effective for non-linear data and handles feature interactions well."
      ],
      "metadata": {
        "id": "5EfbBlXUhuG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "scores = [mae_rf, rmse_rf, r2_rf]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(metrics, scores, color=['skyblue', 'orange', 'green'])\n",
        "plt.title('Random Forest Regressor - Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n",
        "                           param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='r2')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions after tuning\n",
        "y_pred_best_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mae_best = mean_absolute_error(y_test, y_pred_best_rf)\n",
        "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best_rf))\n",
        "r2_best = r2_score(y_test, y_pred_best_rf)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used **GridSearchCV** to systematically search for the best combination of hyperparameters. It ensures optimal tuning for better generalization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the RÂ² improved slightly from 0.9976 to 0.9981, and both MAE and RMSE decreased, showing improved accuracy and robustness."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAE:** Ensures the model maintains consistent predictions with minimal average error.\n",
        "\n",
        "**RMSE:** Penalizes large errors that could lead to major financial misjudgments.\n",
        "\n",
        "**RÂ² Score:** Indicates how well the model explains market trends â crucial for decision-making."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Random Forest Regressor (after tuning) as the final model due to:\n",
        "\n",
        "- Highest RÂ² score (â 0.9981)\n",
        "\n",
        "- Lowest MAE and RMSE\n",
        "\n",
        "- Robustness to overfitting\n",
        "\n",
        "- Superior handling of non-linear relationships"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights:**\n",
        "\n",
        "Open, High, and Low had the highest impact on the prediction.\n",
        "\n",
        "Time features like Month and Day had lesser but notable influence â useful for seasonal trends."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance Visualization\n",
        "import seaborn as sns\n",
        "\n",
        "importance = best_rf_model.feature_importances_\n",
        "features = X_train.columns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=importance, y=features)\n",
        "plt.title('Feature Importance - Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QxCHCPj5idcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the Best Performing ML Model (Random Forest) using Pickle\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "\n",
        "# Save the model to a file\n",
        "with open('best_rf_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_rf_model, file)"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load the Model and Predict on Unseen Data\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# Load the saved model\n",
        "with open('best_rf_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Sanity check with unseen test data\n",
        "unseen_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Example: Print a few predictions vs actuals\n",
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Actual': y_test[:5].values,\n",
        "    'Predicted': unseen_pred[:5]\n",
        "})\n",
        "print(results)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully predicted the monthly closing stock price of Yes Bank using historical data. The process involved multiple stages: data preprocessing, feature engineering, transformation, scaling, splitting, and applying multiple machine learning models.\n",
        "\n",
        "Three ML models â **Linear Regression**, **Decision Tree**, and **Random Forest** were implemented and evaluated using performance metrics such as **MAE**, **RMSE**, and **RÂ²** Score. Among them, the Random Forest Regressor delivered the best performance with a high RÂ² score and the lowest prediction errors, making it the final chosen model.\n",
        "\n",
        "We further enhanced performance using **GridSearchCV** for **hyperparameter tuning** and ensured model **reliability** through cross-validation. The model was also saved using Pickle and successfully reloaded to predict on unseen data.\n",
        "\n",
        "The project demonstrates the power of ML in financial forecasting, supporting better decision-making through accurate predictions of stock prices. This solution is scalable and can be extended to real-time or larger financial datasets."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}